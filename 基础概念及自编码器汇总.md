# 基础概念及自编码器汇总

### KL散度：

比较两个概率分布相似程度的概念，对于一些实验，会选择一些简单的概率分布去近似复杂的概率分布，KL散度表示我们选择的简单概率分布和原来复杂概率分布的损失值![image-20231103104443666](C:\Users\10925\AppData\Roaming\Typora\typora-user-images\image-20231103104443666.png)

比如在之前的介绍PPO论文里，文中将KL作为一个惩罚项，此时的KL项就代表了新旧策略概率分布的差异程度。而在这里，我们将KL当作一个惩罚项来看待，PPO的思想希望其新旧策略的差距不要过大，这会导致神经网络不好收敛，所以使用β参数自适应的去调整KL，以防止其概率分布差距过大。![image-20231103105220273](C:\Users\10925\AppData\Roaming\Typora\typora-user-images\image-20231103105220273.png)

### 损失函数和目标函数：

目标函数：机器学习算法最后大部分可以归结为求解最优化问题，以达到我们想让算法达到的目标。为了完成某一目标，需要构造出一个“目标函数”来，然后让该函数取极大值或极小值，从而得到机器学习算法的模型参数。

损失函数：表示我们模型预测值与真实值之间的差异，例如平方损失函数等。

损失函数和目标函数的关系：
损失函数可以是目标函数，例如在一些回归问题中，我们希望预测值和真实值越接近越好，所以我们可以将预测值与真实值的函数（也就是损失函数）作为目标函数，将其最小化是我们的目标。

损失函数也可以不是目标函数：例如在上面的例子中，将损失函数作为我们的目标函数的话，可能会有过拟合的现象出现，所以我们会在目标函数中加入惩罚项，使其泛化能力更好。此时目标函数=损失函数+惩罚项

### 惩罚项：

在机器学习中，为了保证神经网络不要过拟合，往往需要加入惩罚项，通过调整惩罚系数，使得模型的预测能力和泛化能力达到一个平衡，惩罚系数越大，对误差的调整作用也就越强，这样可以使得模型更加简单。但模型过于简单。![image-20231103105220273](C:\Users\10925\AppData\Roaming\Typora\typora-user-images\image-20231103105220273.png)

同样，在自适应β版本的PPO算法中，希望目标函数尽可能大L（），β充当惩罚系数，当d很小时，证明其新旧策略差距不大，这个结果是我们所希望的，所以降低惩罚项的系数，而当新旧策略差距过大，则d的值会增加很多，所以我们在此要增加惩罚项系数。![image-20231103121220719](C:\Users\10925\AppData\Roaming\Typora\typora-user-images\image-20231103121220719.png)

常见的惩罚项有范数惩罚，噪声，dropout等

### 梯度

梯度是一个**向量**，向量的方向是方向导数取得最大值的方向，自变量沿着该方向变化，使函数值变化最大，机器学习中，函数值就是我们的优化目标。当函数值是损失函数值的时候，我们期望的是最小值，这个时候取梯度的反方向，使其损失函数向变小的方向变化量最大,也就是梯度下降。通过这种方法更新参数，可以让损失函数值最快地减小。

### 随机梯度下降（SGD）：

**传统梯度下降**

它是指在**每一次迭代时**使用**所有样本**来进行梯度的更新，在更新过程中，全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向，计算过程由于需要遍历所有样本，所以耗时会比较长。而且，数据集中势必会存在一些对参数更新没有作用的点，这是其缺点。

**随机梯度下降**

是在**每次迭代时**使用**一个样本**来对参数进行更新，在每轮迭代中，随机优化某一条训练数据上的损失函数，参数的更新速度就大大加快了。从迭代的次数上来看，SGD迭代的次数较多，因为每次选取一个样本，不能代表全部样本的趋势，所以其有可能会在最小值附近波动。



**小批量梯度下降（mini-batch）**

**使用一个以上而又不是全部的训练样本**在算法的每一步，我们从具有 很多个样本的**训练集（已经打乱样本的顺序）**中**随机抽出**一小批量(mini-batch)样本  。其数目是一个相对较小的数。重要的是，当训练集大小不固定时，这个小批量样本的数目通常是固定的。我们可能在拟合几十亿的样本时，每次更新计算只用到几百个样本。在目前的机器学习中，我们基本是使用小批量的梯度下降算法。批量的大小需要确定，较大的批量会带来更精确的梯度，但时间成本会增加，批量低于某个数值时，计算时间不会减少，同时效果也会变差。在一定范围内，一般来说批量数目越大，其确定的下降方向越准，引起训练震荡越小。跑完一次 epoch所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

在PPO的论文中，其也是利用小批量的随机梯度下降算法去收敛的。





## 变分自编码器应用异常检测



**异常检测**：异常数据的点是与其他数据显著不同的数据点。观测结果与其他观测结果相差甚远，以至于引起人们怀疑它是由不同的机制产生的。

**常用异常检测方法方法**：

##### 1.基于统计：

是根据指定的概率分布建模，数据点从模型生成的概率低于某一阈值，则该数据点被定义为异常。

##### 2.基于近似：

认为异常数据与其他正常数据存在隔离。其中又包含三种方法，分别是基于聚类的，基于密度的，基于距离的。

**基于聚类**：

对数据应用聚类算法，将数据分为不同的密集区域，评估数据点与每个密集区域的关系，异常评分的标准比如可以将数据点到密集区域质心的距离作为参考标准，其到达质心的距离超过所设置的阈值时，可以将其作为异常。

**基于密度**:

位于数据稀疏区域的数据点，我们可以将其定义为异常

**基于距离**：

与给定数据点的相邻数据点相关的距离测量值，距离超过一定的阈值时，定义其为异常点。

##### 3.基于偏差：

使用降维方法重构数据，如主成分分析或自动编码器，并测量其原始数据点与重构数据点之间的差值，得到重构误差，该误差可作为异常评分。



我对于异常检测的理解：

因为正常数据其表现是一致的，他们大都有相同的特性，出现异常数据的种类很多，而且千奇百怪，我们不可能考虑到每一种出现异常的种类，并且，异常数据的样本相比于正常数据非常少，所以使用自编码器，它用正常数据来训练的原因也正因为如此，通过训练正常数据，得到正常数据的主要特征，当异常进入自编码器后，其特征往往和正常数据差别较大，因此损失误差也很大，我们将其判定为异常。



本篇论文作者提出了一种基于变分编码器的将重构概率用于判定异常数据的方法。



与传统的自编码器的区别：

1.基于重构概率而不是基于重构误差

在自编码器中，隐变量是输入数据的一个降维，而变分自编码器是输入的一个概率分布，考虑了潜在变量空间的可变性，隐变量是一个随机性变量，自编码器中则是则是确定性的映射，这样的变化扩展了VAE的表达能力

2.重构概率不仅考虑重构与原始输入的差异，而且通过考虑分布函数的方差参数来考虑重构的可变性，方差较大的变量会容忍重构与原始数据存在较大差异，作为正常行为;方差较小的变量会显著降低重构概率，这也说明了VAE的对于正常数据的判断范围更广。

3.确定重建误差的阈值要比确定重建概率的阈值困难。由于其生成特性，也可以导出数据的重建来分析异常的根本原因。





#### 算法

![image-20231103185730537](C:\Users\10925\AppData\Roaming\Typora\typora-user-images\image-20231103185730537.png)

将隐变量是近似的去服从正态分布。计算方差和均值。

#### 结论

在比较其余常用的异常检测方法，例如PCA,KPCA,自编码器，显示，变分自编码取得了优异的成果

![image-20231103191036408](C:\Users\10925\AppData\Roaming\Typora\typora-user-images\image-20231103191036408.png)

此图表示在手写数字上的异常检测结果，除了在数字1，7，9之外（效果不好可能是数据本身的原因，因为其他异常检测方法也效果不佳），效果均很好，而且优于其他算法。



